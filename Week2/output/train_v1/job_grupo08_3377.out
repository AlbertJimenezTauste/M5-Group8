Metadata(name='KITTI_train', thing_classes=['Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare'])
[32m[03/04 15:29:04 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=10, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)
    )
  )
)
[32m[03/04 15:30:18 d2.data.build]: [0mRemoved 0 images with no usable annotations. 5985 images left.
[32m[03/04 15:30:19 d2.data.build]: [0mDistribution of instances among all 9 categories:
[36m|  category  | #instances   |   category    | #instances   |  category  | #instances   |
|:----------:|:-------------|:-------------:|:-------------|:----------:|:-------------|
|    Car     | 23072        |      Van      | 2354         |   Truck    | 896          |
| Pedestrian | 3591         | Person_sitt.. | 192          |  Cyclist   | 1321         |
|    Tram    | 412          |     Misc      | 779          |  DontCare  | 9103         |
|            |              |               |              |            |              |
|   total    | 41720        |               |              |            |              |[0m
[32m[03/04 15:30:19 d2.data.common]: [0mSerializing 5985 elements to byte tensors and concatenating them all ...
[32m[03/04 15:30:19 d2.data.common]: [0mSerialized dataset takes 3.47 MiB
[32m[03/04 15:30:19 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[03/04 15:30:19 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[03/04 15:30:20 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[03/04 15:30:32 d2.utils.events]: [0m eta: 0:09:11  iter: 19  total_loss: 3.302  loss_cls: 2.308  loss_box_reg: 0.859  loss_rpn_cls: 0.058  loss_rpn_loc: 0.064  time: 0.3818  data_time: 0.0141  lr: 0.000005  max_mem: 2338M
[32m[03/04 15:30:37 d2.utils.events]: [0m eta: 0:08:53  iter: 39  total_loss: 3.248  loss_cls: 2.212  loss_box_reg: 0.901  loss_rpn_cls: 0.056  loss_rpn_loc: 0.080  time: 0.3228  data_time: 0.0043  lr: 0.000010  max_mem: 2338M
[32m[03/04 15:30:43 d2.utils.events]: [0m eta: 0:08:43  iter: 59  total_loss: 2.975  loss_cls: 2.033  loss_box_reg: 0.872  loss_rpn_cls: 0.048  loss_rpn_loc: 0.050  time: 0.3040  data_time: 0.0043  lr: 0.000015  max_mem: 2338M
[32m[03/04 15:30:48 d2.utils.events]: [0m eta: 0:08:38  iter: 79  total_loss: 2.773  loss_cls: 1.753  loss_box_reg: 0.851  loss_rpn_cls: 0.050  loss_rpn_loc: 0.067  time: 0.2950  data_time: 0.0038  lr: 0.000020  max_mem: 2338M
[32m[03/04 15:30:53 d2.utils.events]: [0m eta: 0:08:32  iter: 99  total_loss: 2.409  loss_cls: 1.410  loss_box_reg: 0.893  loss_rpn_cls: 0.053  loss_rpn_loc: 0.054  time: 0.2898  data_time: 0.0041  lr: 0.000025  max_mem: 2338M
[32m[03/04 15:30:59 d2.utils.events]: [0m eta: 0:08:27  iter: 119  total_loss: 2.021  loss_cls: 1.062  loss_box_reg: 0.836  loss_rpn_cls: 0.050  loss_rpn_loc: 0.052  time: 0.2865  data_time: 0.0035  lr: 0.000030  max_mem: 2338M
[32m[03/04 15:31:04 d2.utils.events]: [0m eta: 0:08:22  iter: 139  total_loss: 1.926  loss_cls: 0.876  loss_box_reg: 0.854  loss_rpn_cls: 0.045  loss_rpn_loc: 0.075  time: 0.2841  data_time: 0.0035  lr: 0.000035  max_mem: 2338M
[32m[03/04 15:31:10 d2.utils.events]: [0m eta: 0:08:17  iter: 159  total_loss: 1.764  loss_cls: 0.755  loss_box_reg: 0.859  loss_rpn_cls: 0.050  loss_rpn_loc: 0.062  time: 0.2824  data_time: 0.0034  lr: 0.000040  max_mem: 2338M
[32m[03/04 15:31:15 d2.utils.events]: [0m eta: 0:08:12  iter: 179  total_loss: 1.680  loss_cls: 0.724  loss_box_reg: 0.815  loss_rpn_cls: 0.044  loss_rpn_loc: 0.048  time: 0.2813  data_time: 0.0038  lr: 0.000045  max_mem: 2338M
[32m[03/04 15:31:21 d2.utils.events]: [0m eta: 0:08:07  iter: 199  total_loss: 1.751  loss_cls: 0.710  loss_box_reg: 0.842  loss_rpn_cls: 0.075  loss_rpn_loc: 0.072  time: 0.2806  data_time: 0.0044  lr: 0.000050  max_mem: 2338M
[32m[03/04 15:31:26 d2.utils.events]: [0m eta: 0:08:02  iter: 219  total_loss: 1.561  loss_cls: 0.608  loss_box_reg: 0.850  loss_rpn_cls: 0.055  loss_rpn_loc: 0.051  time: 0.2808  data_time: 0.0036  lr: 0.000055  max_mem: 2338M
[32m[03/04 15:31:32 d2.utils.events]: [0m eta: 0:07:57  iter: 239  total_loss: 1.505  loss_cls: 0.562  loss_box_reg: 0.808  loss_rpn_cls: 0.052  loss_rpn_loc: 0.088  time: 0.2806  data_time: 0.0037  lr: 0.000060  max_mem: 2338M
[32m[03/04 15:31:38 d2.utils.events]: [0m eta: 0:07:52  iter: 259  total_loss: 1.555  loss_cls: 0.596  loss_box_reg: 0.805  loss_rpn_cls: 0.045  loss_rpn_loc: 0.061  time: 0.2805  data_time: 0.0043  lr: 0.000065  max_mem: 2338M
[32m[03/04 15:31:43 d2.utils.events]: [0m eta: 0:07:47  iter: 279  total_loss: 1.518  loss_cls: 0.626  loss_box_reg: 0.806  loss_rpn_cls: 0.040  loss_rpn_loc: 0.062  time: 0.2804  data_time: 0.0040  lr: 0.000070  max_mem: 2338M
[32m[03/04 15:31:49 d2.utils.events]: [0m eta: 0:07:42  iter: 299  total_loss: 1.481  loss_cls: 0.564  loss_box_reg: 0.811  loss_rpn_cls: 0.048  loss_rpn_loc: 0.069  time: 0.2803  data_time: 0.0039  lr: 0.000075  max_mem: 2338M
[32m[03/04 15:31:54 d2.utils.events]: [0m eta: 0:07:37  iter: 319  total_loss: 1.528  loss_cls: 0.591  loss_box_reg: 0.794  loss_rpn_cls: 0.042  loss_rpn_loc: 0.069  time: 0.2802  data_time: 0.0052  lr: 0.000080  max_mem: 2338M
[32m[03/04 15:32:00 d2.utils.events]: [0m eta: 0:07:33  iter: 339  total_loss: 1.379  loss_cls: 0.459  loss_box_reg: 0.742  loss_rpn_cls: 0.050  loss_rpn_loc: 0.068  time: 0.2801  data_time: 0.0048  lr: 0.000085  max_mem: 2338M
[32m[03/04 15:32:06 d2.utils.events]: [0m eta: 0:07:27  iter: 359  total_loss: 1.298  loss_cls: 0.469  loss_box_reg: 0.738  loss_rpn_cls: 0.050  loss_rpn_loc: 0.047  time: 0.2802  data_time: 0.0040  lr: 0.000090  max_mem: 2338M
[32m[03/04 15:32:11 d2.utils.events]: [0m eta: 0:07:23  iter: 379  total_loss: 1.419  loss_cls: 0.547  loss_box_reg: 0.740  loss_rpn_cls: 0.043  loss_rpn_loc: 0.059  time: 0.2803  data_time: 0.0040  lr: 0.000095  max_mem: 2338M
[32m[03/04 15:32:17 d2.utils.events]: [0m eta: 0:07:18  iter: 399  total_loss: 1.275  loss_cls: 0.480  loss_box_reg: 0.691  loss_rpn_cls: 0.038  loss_rpn_loc: 0.051  time: 0.2802  data_time: 0.0035  lr: 0.000100  max_mem: 2338M
[32m[03/04 15:32:22 d2.utils.events]: [0m eta: 0:07:13  iter: 419  total_loss: 1.231  loss_cls: 0.453  loss_box_reg: 0.663  loss_rpn_cls: 0.032  loss_rpn_loc: 0.065  time: 0.2800  data_time: 0.0038  lr: 0.000105  max_mem: 2338M
[32m[03/04 15:32:28 d2.utils.events]: [0m eta: 0:07:08  iter: 439  total_loss: 1.254  loss_cls: 0.473  loss_box_reg: 0.673  loss_rpn_cls: 0.027  loss_rpn_loc: 0.055  time: 0.2800  data_time: 0.0036  lr: 0.000110  max_mem: 2338M
[32m[03/04 15:32:34 d2.utils.events]: [0m eta: 0:07:02  iter: 459  total_loss: 1.114  loss_cls: 0.434  loss_box_reg: 0.574  loss_rpn_cls: 0.036  loss_rpn_loc: 0.051  time: 0.2801  data_time: 0.0037  lr: 0.000115  max_mem: 2338M
[32m[03/04 15:32:39 d2.utils.events]: [0m eta: 0:06:57  iter: 479  total_loss: 1.087  loss_cls: 0.413  loss_box_reg: 0.596  loss_rpn_cls: 0.025  loss_rpn_loc: 0.044  time: 0.2801  data_time: 0.0040  lr: 0.000120  max_mem: 2338M
[32m[03/04 15:32:45 d2.utils.events]: [0m eta: 0:06:52  iter: 499  total_loss: 1.152  loss_cls: 0.423  loss_box_reg: 0.568  loss_rpn_cls: 0.039  loss_rpn_loc: 0.059  time: 0.2801  data_time: 0.0038  lr: 0.000125  max_mem: 2338M
[32m[03/04 15:32:51 d2.utils.events]: [0m eta: 0:06:47  iter: 519  total_loss: 1.153  loss_cls: 0.447  loss_box_reg: 0.590  loss_rpn_cls: 0.044  loss_rpn_loc: 0.052  time: 0.2803  data_time: 0.0046  lr: 0.000130  max_mem: 2338M
[32m[03/04 15:32:56 d2.utils.events]: [0m eta: 0:06:41  iter: 539  total_loss: 1.174  loss_cls: 0.474  loss_box_reg: 0.567  loss_rpn_cls: 0.050  loss_rpn_loc: 0.062  time: 0.2804  data_time: 0.0042  lr: 0.000135  max_mem: 2338M
[32m[03/04 15:33:02 d2.utils.events]: [0m eta: 0:06:36  iter: 559  total_loss: 1.082  loss_cls: 0.413  loss_box_reg: 0.570  loss_rpn_cls: 0.025  loss_rpn_loc: 0.054  time: 0.2804  data_time: 0.0036  lr: 0.000140  max_mem: 2338M
[32m[03/04 15:33:07 d2.utils.events]: [0m eta: 0:06:30  iter: 579  total_loss: 1.160  loss_cls: 0.488  loss_box_reg: 0.599  loss_rpn_cls: 0.025  loss_rpn_loc: 0.059  time: 0.2804  data_time: 0.0038  lr: 0.000145  max_mem: 2338M
[32m[03/04 15:33:13 d2.utils.events]: [0m eta: 0:06:25  iter: 599  total_loss: 1.072  loss_cls: 0.374  loss_box_reg: 0.557  loss_rpn_cls: 0.023  loss_rpn_loc: 0.067  time: 0.2804  data_time: 0.0038  lr: 0.000150  max_mem: 2338M
[32m[03/04 15:33:19 d2.utils.events]: [0m eta: 0:06:20  iter: 619  total_loss: 0.979  loss_cls: 0.373  loss_box_reg: 0.549  loss_rpn_cls: 0.025  loss_rpn_loc: 0.037  time: 0.2803  data_time: 0.0038  lr: 0.000155  max_mem: 2338M
[32m[03/04 15:33:24 d2.utils.events]: [0m eta: 0:06:15  iter: 639  total_loss: 0.957  loss_cls: 0.353  loss_box_reg: 0.546  loss_rpn_cls: 0.030  loss_rpn_loc: 0.048  time: 0.2803  data_time: 0.0040  lr: 0.000160  max_mem: 2338M
[32m[03/04 15:33:30 d2.utils.events]: [0m eta: 0:06:09  iter: 659  total_loss: 1.065  loss_cls: 0.420  loss_box_reg: 0.557  loss_rpn_cls: 0.025  loss_rpn_loc: 0.044  time: 0.2803  data_time: 0.0038  lr: 0.000165  max_mem: 2338M
[32m[03/04 15:33:36 d2.utils.events]: [0m eta: 0:06:04  iter: 679  total_loss: 1.017  loss_cls: 0.389  loss_box_reg: 0.562  loss_rpn_cls: 0.030  loss_rpn_loc: 0.059  time: 0.2804  data_time: 0.0036  lr: 0.000170  max_mem: 2338M
[32m[03/04 15:33:41 d2.utils.events]: [0m eta: 0:05:59  iter: 699  total_loss: 1.131  loss_cls: 0.408  loss_box_reg: 0.594  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.2805  data_time: 0.0046  lr: 0.000175  max_mem: 2338M
[32m[03/04 15:33:47 d2.utils.events]: [0m eta: 0:05:53  iter: 719  total_loss: 1.044  loss_cls: 0.409  loss_box_reg: 0.561  loss_rpn_cls: 0.034  loss_rpn_loc: 0.061  time: 0.2805  data_time: 0.0040  lr: 0.000180  max_mem: 2338M
[32m[03/04 15:33:53 d2.utils.events]: [0m eta: 0:05:48  iter: 739  total_loss: 0.969  loss_cls: 0.319  loss_box_reg: 0.480  loss_rpn_cls: 0.026  loss_rpn_loc: 0.045  time: 0.2807  data_time: 0.0043  lr: 0.000185  max_mem: 2338M
[32m[03/04 15:33:58 d2.utils.events]: [0m eta: 0:05:43  iter: 759  total_loss: 1.011  loss_cls: 0.359  loss_box_reg: 0.522  loss_rpn_cls: 0.018  loss_rpn_loc: 0.056  time: 0.2808  data_time: 0.0042  lr: 0.000190  max_mem: 2338M
[32m[03/04 15:34:04 d2.utils.events]: [0m eta: 0:05:37  iter: 779  total_loss: 0.996  loss_cls: 0.358  loss_box_reg: 0.525  loss_rpn_cls: 0.029  loss_rpn_loc: 0.065  time: 0.2807  data_time: 0.0038  lr: 0.000195  max_mem: 2338M
[32m[03/04 15:34:10 d2.utils.events]: [0m eta: 0:05:32  iter: 799  total_loss: 1.030  loss_cls: 0.377  loss_box_reg: 0.538  loss_rpn_cls: 0.032  loss_rpn_loc: 0.054  time: 0.2808  data_time: 0.0038  lr: 0.000200  max_mem: 2338M
[32m[03/04 15:34:15 d2.utils.events]: [0m eta: 0:05:27  iter: 819  total_loss: 0.995  loss_cls: 0.355  loss_box_reg: 0.508  loss_rpn_cls: 0.033  loss_rpn_loc: 0.051  time: 0.2808  data_time: 0.0040  lr: 0.000205  max_mem: 2338M
[32m[03/04 15:34:21 d2.utils.events]: [0m eta: 0:05:21  iter: 839  total_loss: 1.075  loss_cls: 0.356  loss_box_reg: 0.619  loss_rpn_cls: 0.027  loss_rpn_loc: 0.049  time: 0.2808  data_time: 0.0038  lr: 0.000210  max_mem: 2338M
[32m[03/04 15:34:27 d2.utils.events]: [0m eta: 0:05:16  iter: 859  total_loss: 0.937  loss_cls: 0.339  loss_box_reg: 0.483  loss_rpn_cls: 0.024  loss_rpn_loc: 0.050  time: 0.2808  data_time: 0.0041  lr: 0.000215  max_mem: 2338M
[32m[03/04 15:34:32 d2.utils.events]: [0m eta: 0:05:10  iter: 879  total_loss: 1.043  loss_cls: 0.371  loss_box_reg: 0.546  loss_rpn_cls: 0.025  loss_rpn_loc: 0.071  time: 0.2808  data_time: 0.0038  lr: 0.000220  max_mem: 2338M
[32m[03/04 15:34:38 d2.utils.events]: [0m eta: 0:05:05  iter: 899  total_loss: 0.987  loss_cls: 0.340  loss_box_reg: 0.537  loss_rpn_cls: 0.033  loss_rpn_loc: 0.049  time: 0.2808  data_time: 0.0035  lr: 0.000225  max_mem: 2338M
[32m[03/04 15:34:43 d2.utils.events]: [0m eta: 0:04:59  iter: 919  total_loss: 0.869  loss_cls: 0.299  loss_box_reg: 0.463  loss_rpn_cls: 0.026  loss_rpn_loc: 0.056  time: 0.2808  data_time: 0.0037  lr: 0.000230  max_mem: 2338M
[32m[03/04 15:34:49 d2.utils.events]: [0m eta: 0:04:54  iter: 939  total_loss: 0.996  loss_cls: 0.376  loss_box_reg: 0.531  loss_rpn_cls: 0.037  loss_rpn_loc: 0.066  time: 0.2808  data_time: 0.0040  lr: 0.000235  max_mem: 2338M
[32m[03/04 15:34:55 d2.utils.events]: [0m eta: 0:04:48  iter: 959  total_loss: 0.981  loss_cls: 0.330  loss_box_reg: 0.544  loss_rpn_cls: 0.035  loss_rpn_loc: 0.041  time: 0.2809  data_time: 0.0051  lr: 0.000240  max_mem: 2338M
[32m[03/04 15:35:00 d2.utils.events]: [0m eta: 0:04:43  iter: 979  total_loss: 0.864  loss_cls: 0.346  loss_box_reg: 0.496  loss_rpn_cls: 0.028  loss_rpn_loc: 0.051  time: 0.2810  data_time: 0.0043  lr: 0.000245  max_mem: 2338M
[32m[03/04 15:35:06 d2.utils.events]: [0m eta: 0:04:37  iter: 999  total_loss: 0.950  loss_cls: 0.317  loss_box_reg: 0.490  loss_rpn_cls: 0.030  loss_rpn_loc: 0.059  time: 0.2810  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:35:12 d2.utils.events]: [0m eta: 0:04:32  iter: 1019  total_loss: 1.087  loss_cls: 0.377  loss_box_reg: 0.527  loss_rpn_cls: 0.037  loss_rpn_loc: 0.084  time: 0.2810  data_time: 0.0037  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:35:17 d2.utils.events]: [0m eta: 0:04:26  iter: 1039  total_loss: 0.889  loss_cls: 0.315  loss_box_reg: 0.445  loss_rpn_cls: 0.041  loss_rpn_loc: 0.057  time: 0.2810  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:35:23 d2.utils.events]: [0m eta: 0:04:21  iter: 1059  total_loss: 0.992  loss_cls: 0.329  loss_box_reg: 0.485  loss_rpn_cls: 0.028  loss_rpn_loc: 0.056  time: 0.2810  data_time: 0.0041  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:35:29 d2.utils.events]: [0m eta: 0:04:16  iter: 1079  total_loss: 0.807  loss_cls: 0.269  loss_box_reg: 0.445  loss_rpn_cls: 0.025  loss_rpn_loc: 0.035  time: 0.2810  data_time: 0.0035  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:35:34 d2.utils.events]: [0m eta: 0:04:10  iter: 1099  total_loss: 0.843  loss_cls: 0.307  loss_box_reg: 0.460  loss_rpn_cls: 0.031  loss_rpn_loc: 0.050  time: 0.2810  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:35:40 d2.utils.events]: [0m eta: 0:04:05  iter: 1119  total_loss: 0.915  loss_cls: 0.344  loss_box_reg: 0.462  loss_rpn_cls: 0.031  loss_rpn_loc: 0.049  time: 0.2811  data_time: 0.0043  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:35:46 d2.utils.events]: [0m eta: 0:04:00  iter: 1139  total_loss: 0.967  loss_cls: 0.293  loss_box_reg: 0.460  loss_rpn_cls: 0.042  loss_rpn_loc: 0.066  time: 0.2812  data_time: 0.0043  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:35:51 d2.utils.events]: [0m eta: 0:03:54  iter: 1159  total_loss: 0.814  loss_cls: 0.273  loss_box_reg: 0.424  loss_rpn_cls: 0.036  loss_rpn_loc: 0.030  time: 0.2812  data_time: 0.0042  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:35:57 d2.utils.events]: [0m eta: 0:03:49  iter: 1179  total_loss: 0.871  loss_cls: 0.323  loss_box_reg: 0.465  loss_rpn_cls: 0.027  loss_rpn_loc: 0.041  time: 0.2812  data_time: 0.0033  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:03 d2.utils.events]: [0m eta: 0:03:43  iter: 1199  total_loss: 0.924  loss_cls: 0.319  loss_box_reg: 0.477  loss_rpn_cls: 0.032  loss_rpn_loc: 0.077  time: 0.2812  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:08 d2.utils.events]: [0m eta: 0:03:38  iter: 1219  total_loss: 0.922  loss_cls: 0.321  loss_box_reg: 0.488  loss_rpn_cls: 0.029  loss_rpn_loc: 0.049  time: 0.2811  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:14 d2.utils.events]: [0m eta: 0:03:32  iter: 1239  total_loss: 0.814  loss_cls: 0.299  loss_box_reg: 0.443  loss_rpn_cls: 0.025  loss_rpn_loc: 0.040  time: 0.2812  data_time: 0.0040  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:20 d2.utils.events]: [0m eta: 0:03:26  iter: 1259  total_loss: 0.819  loss_cls: 0.291  loss_box_reg: 0.430  loss_rpn_cls: 0.029  loss_rpn_loc: 0.051  time: 0.2812  data_time: 0.0040  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:25 d2.utils.events]: [0m eta: 0:03:21  iter: 1279  total_loss: 0.856  loss_cls: 0.290  loss_box_reg: 0.468  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.2812  data_time: 0.0040  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:31 d2.utils.events]: [0m eta: 0:03:15  iter: 1299  total_loss: 0.799  loss_cls: 0.264  loss_box_reg: 0.405  loss_rpn_cls: 0.020  loss_rpn_loc: 0.035  time: 0.2812  data_time: 0.0044  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:37 d2.utils.events]: [0m eta: 0:03:10  iter: 1319  total_loss: 0.755  loss_cls: 0.243  loss_box_reg: 0.446  loss_rpn_cls: 0.021  loss_rpn_loc: 0.043  time: 0.2812  data_time: 0.0045  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:42 d2.utils.events]: [0m eta: 0:03:04  iter: 1339  total_loss: 0.818  loss_cls: 0.254  loss_box_reg: 0.447  loss_rpn_cls: 0.028  loss_rpn_loc: 0.045  time: 0.2812  data_time: 0.0037  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:48 d2.utils.events]: [0m eta: 0:02:59  iter: 1359  total_loss: 0.857  loss_cls: 0.296  loss_box_reg: 0.470  loss_rpn_cls: 0.025  loss_rpn_loc: 0.050  time: 0.2812  data_time: 0.0037  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:53 d2.utils.events]: [0m eta: 0:02:53  iter: 1379  total_loss: 0.812  loss_cls: 0.263  loss_box_reg: 0.429  loss_rpn_cls: 0.031  loss_rpn_loc: 0.052  time: 0.2811  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:36:59 d2.utils.events]: [0m eta: 0:02:47  iter: 1399  total_loss: 0.912  loss_cls: 0.306  loss_box_reg: 0.453  loss_rpn_cls: 0.031  loss_rpn_loc: 0.052  time: 0.2811  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:05 d2.utils.events]: [0m eta: 0:02:42  iter: 1419  total_loss: 0.763  loss_cls: 0.224  loss_box_reg: 0.391  loss_rpn_cls: 0.024  loss_rpn_loc: 0.042  time: 0.2811  data_time: 0.0035  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:10 d2.utils.events]: [0m eta: 0:02:36  iter: 1439  total_loss: 0.691  loss_cls: 0.223  loss_box_reg: 0.351  loss_rpn_cls: 0.021  loss_rpn_loc: 0.041  time: 0.2811  data_time: 0.0037  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:16 d2.utils.events]: [0m eta: 0:02:31  iter: 1459  total_loss: 0.938  loss_cls: 0.345  loss_box_reg: 0.491  loss_rpn_cls: 0.028  loss_rpn_loc: 0.058  time: 0.2811  data_time: 0.0045  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:22 d2.utils.events]: [0m eta: 0:02:25  iter: 1479  total_loss: 0.929  loss_cls: 0.327  loss_box_reg: 0.466  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.2812  data_time: 0.0044  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:27 d2.utils.events]: [0m eta: 0:02:20  iter: 1499  total_loss: 0.809  loss_cls: 0.283  loss_box_reg: 0.442  loss_rpn_cls: 0.031  loss_rpn_loc: 0.059  time: 0.2812  data_time: 0.0047  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:33 d2.utils.events]: [0m eta: 0:02:14  iter: 1519  total_loss: 0.858  loss_cls: 0.290  loss_box_reg: 0.440  loss_rpn_cls: 0.043  loss_rpn_loc: 0.051  time: 0.2812  data_time: 0.0039  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:39 d2.utils.events]: [0m eta: 0:02:08  iter: 1539  total_loss: 0.840  loss_cls: 0.271  loss_box_reg: 0.424  loss_rpn_cls: 0.020  loss_rpn_loc: 0.038  time: 0.2812  data_time: 0.0037  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:44 d2.utils.events]: [0m eta: 0:02:03  iter: 1559  total_loss: 0.781  loss_cls: 0.291  loss_box_reg: 0.416  loss_rpn_cls: 0.023  loss_rpn_loc: 0.055  time: 0.2812  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:50 d2.utils.events]: [0m eta: 0:01:57  iter: 1579  total_loss: 0.810  loss_cls: 0.342  loss_box_reg: 0.437  loss_rpn_cls: 0.022  loss_rpn_loc: 0.043  time: 0.2812  data_time: 0.0035  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:37:55 d2.utils.events]: [0m eta: 0:01:52  iter: 1599  total_loss: 0.784  loss_cls: 0.288  loss_box_reg: 0.428  loss_rpn_cls: 0.030  loss_rpn_loc: 0.053  time: 0.2812  data_time: 0.0034  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:01 d2.utils.events]: [0m eta: 0:01:46  iter: 1619  total_loss: 0.698  loss_cls: 0.245  loss_box_reg: 0.423  loss_rpn_cls: 0.016  loss_rpn_loc: 0.042  time: 0.2813  data_time: 0.0044  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:07 d2.utils.events]: [0m eta: 0:01:40  iter: 1639  total_loss: 0.839  loss_cls: 0.319  loss_box_reg: 0.424  loss_rpn_cls: 0.025  loss_rpn_loc: 0.048  time: 0.2812  data_time: 0.0040  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:12 d2.utils.events]: [0m eta: 0:01:35  iter: 1659  total_loss: 0.750  loss_cls: 0.266  loss_box_reg: 0.426  loss_rpn_cls: 0.019  loss_rpn_loc: 0.029  time: 0.2812  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:18 d2.utils.events]: [0m eta: 0:01:29  iter: 1679  total_loss: 0.859  loss_cls: 0.336  loss_box_reg: 0.424  loss_rpn_cls: 0.033  loss_rpn_loc: 0.066  time: 0.2812  data_time: 0.0037  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:24 d2.utils.events]: [0m eta: 0:01:24  iter: 1699  total_loss: 0.895  loss_cls: 0.329  loss_box_reg: 0.469  loss_rpn_cls: 0.039  loss_rpn_loc: 0.046  time: 0.2812  data_time: 0.0034  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:29 d2.utils.events]: [0m eta: 0:01:18  iter: 1719  total_loss: 0.737  loss_cls: 0.274  loss_box_reg: 0.400  loss_rpn_cls: 0.018  loss_rpn_loc: 0.037  time: 0.2812  data_time: 0.0037  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:35 d2.utils.events]: [0m eta: 0:01:12  iter: 1739  total_loss: 0.788  loss_cls: 0.236  loss_box_reg: 0.406  loss_rpn_cls: 0.024  loss_rpn_loc: 0.056  time: 0.2812  data_time: 0.0040  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:40 d2.utils.events]: [0m eta: 0:01:07  iter: 1759  total_loss: 0.788  loss_cls: 0.304  loss_box_reg: 0.400  loss_rpn_cls: 0.024  loss_rpn_loc: 0.048  time: 0.2812  data_time: 0.0039  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:46 d2.utils.events]: [0m eta: 0:01:01  iter: 1779  total_loss: 0.829  loss_cls: 0.272  loss_box_reg: 0.452  loss_rpn_cls: 0.020  loss_rpn_loc: 0.057  time: 0.2812  data_time: 0.0044  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:52 d2.utils.events]: [0m eta: 0:00:56  iter: 1799  total_loss: 0.785  loss_cls: 0.310  loss_box_reg: 0.426  loss_rpn_cls: 0.021  loss_rpn_loc: 0.049  time: 0.2812  data_time: 0.0045  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:38:57 d2.utils.events]: [0m eta: 0:00:50  iter: 1819  total_loss: 0.731  loss_cls: 0.279  loss_box_reg: 0.390  loss_rpn_cls: 0.031  loss_rpn_loc: 0.047  time: 0.2812  data_time: 0.0045  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:03 d2.utils.events]: [0m eta: 0:00:44  iter: 1839  total_loss: 0.844  loss_cls: 0.310  loss_box_reg: 0.458  loss_rpn_cls: 0.017  loss_rpn_loc: 0.038  time: 0.2812  data_time: 0.0035  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:09 d2.utils.events]: [0m eta: 0:00:39  iter: 1859  total_loss: 0.910  loss_cls: 0.354  loss_box_reg: 0.472  loss_rpn_cls: 0.033  loss_rpn_loc: 0.047  time: 0.2812  data_time: 0.0038  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:14 d2.utils.events]: [0m eta: 0:00:33  iter: 1879  total_loss: 0.833  loss_cls: 0.336  loss_box_reg: 0.418  loss_rpn_cls: 0.026  loss_rpn_loc: 0.062  time: 0.2811  data_time: 0.0035  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:20 d2.utils.events]: [0m eta: 0:00:28  iter: 1899  total_loss: 0.756  loss_cls: 0.283  loss_box_reg: 0.392  loss_rpn_cls: 0.021  loss_rpn_loc: 0.042  time: 0.2811  data_time: 0.0035  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:25 d2.utils.events]: [0m eta: 0:00:22  iter: 1919  total_loss: 0.793  loss_cls: 0.266  loss_box_reg: 0.404  loss_rpn_cls: 0.023  loss_rpn_loc: 0.063  time: 0.2811  data_time: 0.0037  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:31 d2.utils.events]: [0m eta: 0:00:17  iter: 1939  total_loss: 0.835  loss_cls: 0.283  loss_box_reg: 0.454  loss_rpn_cls: 0.027  loss_rpn_loc: 0.047  time: 0.2811  data_time: 0.0040  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:37 d2.utils.events]: [0m eta: 0:00:11  iter: 1959  total_loss: 0.856  loss_cls: 0.317  loss_box_reg: 0.423  loss_rpn_cls: 0.019  loss_rpn_loc: 0.068  time: 0.2812  data_time: 0.0045  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:43 d2.utils.events]: [0m eta: 0:00:05  iter: 1979  total_loss: 0.796  loss_cls: 0.290  loss_box_reg: 0.428  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.2812  data_time: 0.0046  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:53 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.869  loss_cls: 0.301  loss_box_reg: 0.451  loss_rpn_cls: 0.023  loss_rpn_loc: 0.070  time: 0.2812  data_time: 0.0037  lr: 0.000250  max_mem: 2338M
[32m[03/04 15:39:53 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:09:21 (0.2814 s / it)
[32m[03/04 15:39:53 d2.engine.hooks]: [0mTotal training time: 0:09:28 (0:00:06 on hooks)
