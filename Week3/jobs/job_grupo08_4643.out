Metadata(name='MOTSChallenge_train', thing_classes=['None', 'Car', 'Pedestrian'])
Loading sequence 0002
Loading sequence 0009
Loading sequence 0011
0002
0009
0011
Loaded 2025 images!
[5m[31mWARNING[0m [32m[03/15 15:46:23 d2.evaluation.coco_evaluation]: [0mjson_file was not found in MetaDataCatalog for 'MOTSChallenge_val'. Trying to convert it to COCO format ...
[32m[03/15 15:46:23 d2.data.datasets.coco]: [0mConverting dataset annotations in 'MOTSChallenge_val' to COCO format ...)
Loading sequence 0005
0005
Loaded 837 images!
[32m[03/15 15:46:36 d2.data.datasets.coco]: [0mConverting dataset dicts into COCO format
[32m[03/15 15:46:37 d2.data.datasets.coco]: [0mConversion finished, num images: 837, num annotations: 6570
[32m[03/15 15:46:37 d2.data.datasets.coco]: [0mCaching annotations in COCO format: ./output/MOTSChallenge_val_coco_format.json
Loading sequence 0005
0005
Loaded 837 images!
[32m[03/15 15:46:48 d2.data.build]: [0mDistribution of instances among all 3 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    None    | 0            |    Car     | 0            | Pedestrian | 6570         |
|            |              |            |              |            |              |
|   total    | 6570         |            |              |            |              |[0m
[32m[03/15 15:46:48 d2.data.common]: [0mSerializing 837 elements to byte tensors and concatenating them all ...
[32m[03/15 15:46:48 d2.data.common]: [0mSerialized dataset takes 0.84 MiB
[32m[03/15 15:46:48 d2.evaluation.evaluator]: [0mStart inference on 837 images
[32m[03/15 15:46:50 d2.evaluation.evaluator]: [0mInference done 11/837. 0.1111 s / img. ETA=0:01:32
[32m[03/15 15:46:55 d2.evaluation.evaluator]: [0mInference done 54/837. 0.1150 s / img. ETA=0:01:31
[32m[03/15 15:47:00 d2.evaluation.evaluator]: [0mInference done 99/837. 0.1131 s / img. ETA=0:01:24
[32m[03/15 15:47:05 d2.evaluation.evaluator]: [0mInference done 144/837. 0.1123 s / img. ETA=0:01:18
[32m[03/15 15:47:10 d2.evaluation.evaluator]: [0mInference done 189/837. 0.1122 s / img. ETA=0:01:13
[32m[03/15 15:47:15 d2.evaluation.evaluator]: [0mInference done 234/837. 0.1120 s / img. ETA=0:01:08
[32m[03/15 15:47:20 d2.evaluation.evaluator]: [0mInference done 279/837. 0.1119 s / img. ETA=0:01:03
[32m[03/15 15:47:25 d2.evaluation.evaluator]: [0mInference done 324/837. 0.1119 s / img. ETA=0:00:58
[32m[03/15 15:47:30 d2.evaluation.evaluator]: [0mInference done 369/837. 0.1119 s / img. ETA=0:00:53
[32m[03/15 15:47:35 d2.evaluation.evaluator]: [0mInference done 414/837. 0.1119 s / img. ETA=0:00:47
[32m[03/15 15:47:40 d2.evaluation.evaluator]: [0mInference done 459/837. 0.1119 s / img. ETA=0:00:42
[32m[03/15 15:47:45 d2.evaluation.evaluator]: [0mInference done 503/837. 0.1119 s / img. ETA=0:00:37
[32m[03/15 15:47:50 d2.evaluation.evaluator]: [0mInference done 547/837. 0.1120 s / img. ETA=0:00:32
[32m[03/15 15:47:55 d2.evaluation.evaluator]: [0mInference done 591/837. 0.1120 s / img. ETA=0:00:27
[32m[03/15 15:48:01 d2.evaluation.evaluator]: [0mInference done 635/837. 0.1120 s / img. ETA=0:00:22
[32m[03/15 15:48:06 d2.evaluation.evaluator]: [0mInference done 679/837. 0.1121 s / img. ETA=0:00:17
[32m[03/15 15:48:11 d2.evaluation.evaluator]: [0mInference done 723/837. 0.1122 s / img. ETA=0:00:12
[32m[03/15 15:48:16 d2.evaluation.evaluator]: [0mInference done 766/837. 0.1122 s / img. ETA=0:00:08
[32m[03/15 15:48:21 d2.evaluation.evaluator]: [0mInference done 810/837. 0.1122 s / img. ETA=0:00:03
[32m[03/15 15:48:24 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:34.701028 (0.113823 s / img per device, on 1 devices)
[32m[03/15 15:48:24 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:33 (0.112242 s / img per device, on 1 devices)
[32m[03/15 15:48:24 d2.evaluation.coco_evaluation]: [0mPreparing results for COCO format ...
[32m[03/15 15:48:24 d2.evaluation.coco_evaluation]: [0mSaving results to ./output/coco_instances_results.json
[32m[03/15 15:48:24 d2.evaluation.coco_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.17s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=12.08s).
Accumulating evaluation results...
DONE (t=0.56s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.606
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.846
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.669
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.117
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.507
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.777
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.115
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.630
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.688
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.615
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.833
[32m[03/15 15:48:37 d2.evaluation.coco_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 60.617 | 84.606 | 66.917 | 11.662 | 50.683 | 77.747 |
[32m[03/15 15:48:37 d2.evaluation.coco_evaluation]: [0mPer-category bbox AP: 
| category   | AP   | category   | AP   | category   | AP     |
|:-----------|:-----|:-----------|:-----|:-----------|:-------|
| None       | nan  | Car        | nan  | Pedestrian | 60.617 |
